{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time pour ralentir le proccess de scraping\n",
    "import time\n",
    "#Request pour envoyer et recevoir les requette de serveur\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "#Beautifulsoup pour récuperer les balise html et extraire les données\n",
    "from bs4 import BeautifulSoup\n",
    "import mysql\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici j'ai essayé de connecter \n",
    "try:\n",
    "    connection = mysql.connector.connect(host='172.28.100.229',database='projet',user='azad',password='886622')\n",
    "\n",
    "except Error as e:\n",
    "    print(\"err:\", e)\n",
    "\n",
    "#cursor = connection.cursor()\n",
    "#cursor.execute('''DROP TABLE IF EXISTS annonces''')\n",
    "#cursor.execute('''CREATE TABLE annonces (id int AUTO_INCREMENT, motcle VARCHAR(255),metier VARCHAR(255), entreprise VARCHAR(255),location VARCHAR(255),datedannoce VARCHAR(255),lien LONGTEXT,description LONGTEXT, PRIMARY KEY(id));''')\n",
    "\n",
    "#connection.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://fr.indeed.com/emplois?q=data%20analyst&l=Lille\n",
    "#je vais remplacer nom d'emploi et la ville avec {} \n",
    "#pour que je puisse chercher autant ville ou metier que je desire\n",
    "#url_model='https://fr.indeed.com/emplois?q={}&l={}'\n",
    "\n",
    "\n",
    "#Ici j'ai créé une fonctionne afin de retourner le vrai lien en lui applenat avec nom de metier et un lieu\n",
    "def recupere_url(n_metier, lieu):\n",
    "    url_model='https://fr.indeed.com/emplois?q={}&l={}'\n",
    "    vrai_url=url_model.format(n_metier, lieu)\n",
    "    return vrai_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Une fonctionne afin d'ouvrire chaque annonce et scraper sa description\n",
    "\n",
    "def recupere_decription(lien):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36'}\n",
    "    time.sleep(20)\n",
    "    requet = requests.get(lien)\n",
    "    soup_html= BeautifulSoup(requet.text, 'html.parser')\n",
    "    description_job=soup_html.find('div', 'jobsearch-jobDescriptionText')\n",
    "    reponse = description_job.get_text(separator=u' ')\n",
    "    return reponse\n",
    "\n",
    "# Une fonctionne pour récuperer les informations de chaque bloque comme nom de job, nom d'entreprise, \n",
    "# lien de job et son lieu\n",
    "\n",
    "def recupere_donnees(metier):\n",
    "\n",
    "    #Je commence par création d'un dictionnaire afin de regrouper tous infos dont j'ai besoin de chaque métier\n",
    "    donnees = {}\n",
    "    \n",
    "    # Je vais stocker les codes HTML bruts dans une variable que j'ai nommé 'job'\n",
    "    \n",
    "    job = metier\n",
    "    \n",
    "    #Nom de métier est dans un balise <a> qui est dans un autre balise <h2>\n",
    "    #Donc j'utilise le methode get avec atrribut \"title\" pour récuperer le nom de metier\n",
    "    #Et en fin j'ai fait un strip() pour supprimer les spaces et nouveau ligne qui peuvent être avant ou après\n",
    "    #le nom de metier\n",
    "    \n",
    "    #donnees['motcle'] = motcle\n",
    "    donnees['metier'] = job.h2.a.get(\"title\").strip()\n",
    "    \n",
    "\n",
    "    #Nom d'entreprise\n",
    "    try:\n",
    "    \n",
    "        entreprise_n = job.find('span',{'class':'company'})\n",
    "        if(entreprise_n) :\n",
    "            donnees['entreprise'] = str(entreprise_n.get_text()).strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        donnees['entreprise'] = ''\n",
    "    \n",
    "    \n",
    "    #Location de job\n",
    "    try:\n",
    "        \n",
    "        location_main = job.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "        location_div = job.find('div','location')\n",
    "        location_span = job.find('span',{'class':'location'})\n",
    "    \n",
    "        if(location_main) :\n",
    "        \n",
    "            donnees['location'] = location_main\n",
    "        \n",
    "        elif(location_span) :\n",
    "        \n",
    "            donnees['location'] = location_span.get_text()\n",
    "        \n",
    "        elif(location_div) :\n",
    "        \n",
    "            donnees['location'] = location_div2.get_text()\n",
    "    except AttributeError:\n",
    "        \n",
    "        donnees['location'] = ''\n",
    "    \n",
    "    #La date d'annonce\n",
    "    try:\n",
    "        donnees['datedannoce'] = job.find('span',{'class':'date'}).get_text()\n",
    "    except AttributeError:\n",
    "        donnees['datedannoce'] = ''\n",
    "        \n",
    "    \n",
    "    sub_lien = job.h2.a.get('href')\n",
    "    lienDeAnnonce = 'https://fr.indeed.com' + sub_lien\n",
    "    donnees['lien'] = lienDeAnnonce\n",
    "    \n",
    "    try:\n",
    "        donnees['sommaire'] = job.find('div',{'class':'summary'}).text.strip()\n",
    "    except AttributeError:\n",
    "        donnees['sommaire']= ''\n",
    "    #Appeler la fonctionne recupere_description pour obtenier la description d'annonce \n",
    "    #Et le verser dans la dictionnaire\n",
    "    try:\n",
    "        donnees['description'] = recupere_decription(lienDeAnnonce)\n",
    "    except AttributeError:\n",
    "        donnees['description'] = ''\n",
    "    #infos = (nomDeMetier, nomDeEntreprise,location, dateDeAnnonce, salaire, lienDeAnnonce, sommaire, descDeMetier)\n",
    "    #Et à la fin retourner les donnes\n",
    "    \n",
    "    return donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ici j'ai créé une fonctionne pour scrapper la page de resultat de metiers et recuperer les bloques des annonces\n",
    "#n_metier, ville\n",
    "def scrape_lien(n_metier,lieu):\n",
    "\n",
    "    motCle = n_metier\n",
    "\n",
    "    url = recupere_url(n_metier, lieu)\n",
    "    print(url)\n",
    "\n",
    "    #pour éviter d'être bloqué par le site il faut changer le header\n",
    "    while True : \n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'}\n",
    "        \n",
    "        #'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
    "        #ce code va envoyer un requet à site indeed et puis va retourner sa réponse dans la variable reponse que j'ai crée\n",
    "        #Si le process est bien fait la variable reponse va nous retourner 200 qui represent la succes\n",
    "\n",
    "        time.sleep(20)\n",
    "        reponse=requests.get(url,headers=headers,verify=False)\n",
    "        print(reponse)\n",
    "\n",
    "        # maintenant que la reponse est ok je peux récuperer les balises HTML avec \"text\" attribut\n",
    "        #je vais appeler BeautifulSoup pour créer un objet à fin de naviger dans les balises\n",
    "\n",
    "        soup = BeautifulSoup(reponse.text, 'html.parser')\n",
    "        print(soup)\n",
    "\n",
    "        #chaque annoce est dans un balise(bloque_job) div avec le nom de class \"jobsearch-SerpJobCard\"\n",
    "        #qui est en commun avec tous les job annonces\n",
    "        #je vais recuperer identifier et trouver tous les balise avec le nom de class \"jobsearch-SerpJobCard\"\n",
    "        #en utilisant le methode find_all dans beautiful soup\n",
    "\n",
    "        bloques_job = soup.find_all('div', 'jobsearch-SerpJobCard')\n",
    "        print(len(bloques_job))\n",
    "        \n",
    "        for bloque in bloques_job:\n",
    "            donnees = recupere_donnees(bloque)\n",
    "\n",
    "            #print(\"metier : \"+donnees['metier']+\" Nom d'entreprise : \"+donnees['entreprise']+\" location : \" +donnees['location']+\"\\n\")\n",
    "            \n",
    "            cursor2 = connection.cursor()\n",
    "            sql = '''INSERT INTO `newjobs` (`motcle`,`metier`, `entreprise`, `location`, `datedannoce`,`lien`,`sommaire`,`description`) VALUES (%s, %s, %s, %s, %s, %s, %s, %s);'''\n",
    "            cursor2.execute(sql,(motCle, donnees['metier'], donnees['entreprise'],donnees['location'], donnees['datedannoce'], donnees['lien'], donnees['sommaire'],donnees['description'],))\n",
    "            cursor2.close()\n",
    "            connection.commit()\n",
    "    \n",
    "        # Après première page c'est le tour des pages Suivantes\n",
    "        # Sur la premiere page que j'ai escrappé, j'ai remarqué que la deuxieme et les pages suivants sont dans un\n",
    "        # balise <a> avec l'attribut aria-lable et Suivant, \n",
    "        # donc je vais essayer de trouver ce balise dans ma première page\n",
    "        try :\n",
    "            url = 'https://fr.indeed.com' + soup.find('a',{'aria-label':'Suivant'}).get(\"href\")\n",
    "        except AttributeError:\n",
    "\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_metiers=['data analyste','informatique','seo','marketing','python','js','cloud','web']\n",
    "for item in liste_metiers:\n",
    "    scrape_lien(item,'lille')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
